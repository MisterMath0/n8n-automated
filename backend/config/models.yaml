models:
  # Gemini models (Google AI) - Free tier with high context windows
  gemini-2-5-flash:
    name: "Gemini 2.5 Flash"
    provider: "google"
    model_id: "gemini-2.0-flash-exp"
    api_key_env: "GOOGLE_API_KEY"
    max_tokens: 8192
    cost_per_1k_input_tokens: 0.0  # Free tier
    cost_per_1k_output_tokens: 0.0  # Free tier
    supports_json_mode: true
    supports_streaming: true
    supports_tools: true
    context_window: 1000000  # 1M tokens
    default_temperature: 0.3
    
  gemini-1-5-flash:
    name: "Gemini 1.5 Flash"
    provider: "google"
    model_id: "gemini-1.5-flash"
    api_key_env: "GOOGLE_API_KEY"
    max_tokens: 8192
    cost_per_1k_input_tokens: 0.0  # Free tier
    cost_per_1k_output_tokens: 0.0  # Free tier
    supports_json_mode: true
    supports_streaming: true
    supports_tools: true
    context_window: 1000000  # 1M tokens
    default_temperature: 0.3
    
  gemini-1-5-pro:
    name: "Gemini 1.5 Pro"
    provider: "google"
    model_id: "gemini-1.5-pro"
    api_key_env: "GOOGLE_API_KEY"
    max_tokens: 8192
    cost_per_1k_input_tokens: 1.25  # Paid tier
    cost_per_1k_output_tokens: 5.0   # Paid tier
    supports_json_mode: true
    supports_streaming: true
    supports_tools: true
    context_window: 2000000  # 2M tokens
    default_temperature: 0.3

  # Anthropic Claude models
  claude-4-sonnet:
    name: "Claude 4 Sonnet"
    provider: "anthropic"
    model_id: "claude-sonnet-4-20250514"
    api_key_env: "ANTHROPIC_API_KEY"
    max_tokens: 200000
    cost_per_1k_input_tokens: 3.0
    cost_per_1k_output_tokens: 15.0
    supports_json_mode: true
    supports_streaming: true
    supports_tools: true
    context_window: 200000
    default_temperature: 0.3

  claude-4-opus:
    name: "Claude 4 Opus"
    provider: "anthropic"
    model_id: "claude-opus-4-20250514"
    api_key_env: "ANTHROPIC_API_KEY"
    max_tokens: 200000
    cost_per_1k_input_tokens: 15.0
    cost_per_1k_output_tokens: 75.0
    supports_json_mode: true
    supports_streaming: true
    supports_tools: true
    context_window: 200000
    default_temperature: 0.3

  # OpenAI models
  gpt-4-1:
    name: "GPT-4.1"
    provider: "openai"
    model_id: "gpt-4-1"
    base_url: "https://api.openai.com/v1"
    api_key_env: "OPENAI_API_KEY"
    max_tokens: 32768
    cost_per_1k_input_tokens: 2.0 # Assumed per 1M tokens
    cost_per_1k_output_tokens: 8.0 # Assumed per 1M tokens
    supports_json_mode: true
    supports_streaming: true
    supports_tools: true
    context_window: 1047576
    default_temperature: 0.3

  gpt-4-1-mini:
    name: "GPT-4.1 mini"
    provider: "openai"
    model_id: "gpt-4.1-mini"
    base_url: "https://api.openai.com/v1"
    api_key_env: "OPENAI_API_KEY"
    max_tokens: 32768
    cost_per_1k_input_tokens: 0.40 # Assumed per 1M tokens
    cost_per_1k_output_tokens: 1.60 # Assumed per 1M tokens
    supports_json_mode: true
    supports_streaming: true
    supports_tools: true
    context_window: 1047576
    default_temperature: 0.3
  
  gpt-4-1-nano:
    name: "GPT-4.1 nano"
    provider: "openai"
    model_id: "gpt-4.1-nano"
    base_url: "https://api.openai.com/v1"
    api_key_env: "OPENAI_API_KEY"
    max_tokens: 32768
    cost_per_1k_input_tokens: 0.10 # Assumed per 1M tokens
    cost_per_1k_output_tokens: 0.40 # Assumed per 1M tokens
    supports_json_mode: true
    supports_streaming: true
    supports_tools: true
    context_window: 1047576
    default_temperature: 0.3

  # Groq models
  llama-3-3-70b:
    name: "Llama 3.3 70B"
    provider: "groq"
    model_id: "llama-3.3-70b-versatile"
    base_url: "https://api.groq.com/openai/v1"
    api_key_env: "GROQ_API_KEY"
    max_tokens: 32768
    cost_per_1k_input_tokens: 0.5
    cost_per_1k_output_tokens: 0.8
    supports_json_mode: true
    supports_streaming: true
    supports_tools: true
    context_window: 32768
    default_temperature: 0.3

  llama-3-1-8b:
    name: "Llama 3.1 8B"
    provider: "groq"
    model_id: "llama-3.1-8b-instant"
    base_url: "https://api.groq.com/openai/v1"
    api_key_env: "GROQ_API_KEY"
    max_tokens: 8192
    cost_per_1k_input_tokens: 0.05
    cost_per_1k_output_tokens: 0.08
    supports_json_mode: true
    supports_streaming: true
    supports_tools: true
    context_window: 8192
    default_temperature: 0.3

# Default model configuration
default_model: "gemini-2-5-flash"  # Free, high-performance model with 1M context window

# Provider priority (for fallback)
provider_priority:
  - "google"     # Free tier, high context
  - "openai"     # Reliable
  - "anthropic"  # High quality
  - "groq"       # Fast inference  

# Chat-related settings
chat_settings:
  default_context_window: 32000  # Default token limit for chat history
  
  # Per-model overrides for context window (in tokens)
  # Allows for different history limits based on model capabilities
  # For example, smaller models might have a smaller context history
  # to save costs or improve performance.
  model_context_windows:
    "gemini-2-5-flash": 12000
    "gemini-1-5-pro": 16000
    "claude-4-sonnet": 10000
    "gpt-4.1": 12000
