models:
  # Gemini 2.5 Flash - Reasoning Mode (Higher quality, slower)
  gemini-2-5-flash:
    name: "Gemini 2.5 Flash (Reasoning)"
    provider: "google"
    model_id: "gemini-2.5-flash"
    api_key_env: "GOOGLE_API_KEY"
    max_tokens: 8192
    cost_per_1k_input_tokens: 0.0  # Free tier
    cost_per_1k_output_tokens: 0.0  # Free tier
    supports_json_mode: true
    supports_streaming: true
    supports_tools: true
    context_window: 1000000  # 1M tokens
    default_temperature: 0.3
    thinking_config:
      enabled: true
      thinking_budget: 8192  # Allow thinking up to 8K tokens

  # Gemini 2.5 Flash - Fast Mode (No thinking, faster responses)
  gemini-2-5-flash-fast:
    name: "Gemini 2.5 Flash (Fast)"
    provider: "google"
    model_id: "gemini-2.5-flash"
    api_key_env: "GOOGLE_API_KEY"
    max_tokens: 8192
    cost_per_1k_input_tokens: 0.0  # Free tier
    cost_per_1k_output_tokens: 0.0  # Free tier
    supports_json_mode: true
    supports_streaming: true
    supports_tools: true
    context_window: 1000000  # 1M tokens
    default_temperature: 0.3
    thinking_config:
      enabled: false
      thinking_budget: 0  # No thinking for maximum speed

  gpt-4-1-mini:
    name: "GPT-4.1 mini"
    provider: "openai"
    model_id: "gpt-4.1-mini"
    base_url: "https://api.openai.com/v1"
    api_key_env: "OPENAI_API_KEY"
    max_tokens: 32768
    cost_per_1k_input_tokens: 0.40 # Assumed per 1M tokens
    cost_per_1k_output_tokens: 1.60 # Assumed per 1M tokens
    supports_json_mode: true
    supports_streaming: true
    supports_tools: true
    context_window: 1047576
    default_temperature: 0.3
  
  gpt-4-1-nano:
    name: "GPT-4.1 nano"
    provider: "openai"
    model_id: "gpt-4.1-nano"
    base_url: "https://api.openai.com/v1"
    api_key_env: "OPENAI_API_KEY"
    max_tokens: 32768
    cost_per_1k_input_tokens: 0.10 # Assumed per 1M tokens
    cost_per_1k_output_tokens: 0.40 # Assumed per 1M tokens
    supports_json_mode: true
    supports_streaming: true
    supports_tools: true
    context_window: 1047576
    default_temperature: 0.3


# Default model configuration - Use reasoning mode for better workflow generation
default_model: "gemini-2-5-flash"  # Reasoning mode for complex workflow generation

# Provider priority (for fallback)
provider_priority:
  - "google"     # Free tier, high context
  - "openai"     # Reliable
  - "anthropic"  # High quality
  - "groq"       # Fast inference  

# Chat-related settings
chat_settings:
  default_context_window: 32000  # Default token limit for chat history
  
  # Per-model overrides for context window (in tokens)
  # Allows for different history limits based on model capabilities
  # For example, smaller models might have a smaller context history
  # to save costs or improve performance.
  model_context_windows:
    "gemini-2-5-flash": 128000
    "claude-4-sonnet": 10000
    "gpt-4-1-mini": 64000
